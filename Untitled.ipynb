{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/home/parv/Documents/Untitled Folder/train_indessa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>batch_enrolled</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>...</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>mths_since_last_major_derog</th>\n",
       "      <th>application_type</th>\n",
       "      <th>verification_status_joint</th>\n",
       "      <th>last_week_pay</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>tot_coll_amt</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58189336</td>\n",
       "      <td>14350</td>\n",
       "      <td>14350</td>\n",
       "      <td>14350.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td></td>\n",
       "      <td>19.19</td>\n",
       "      <td>E</td>\n",
       "      <td>E3</td>\n",
       "      <td>clerk</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26th week</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28699.0</td>\n",
       "      <td>30800.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70011223</td>\n",
       "      <td>4800</td>\n",
       "      <td>4800</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>BAT1586599</td>\n",
       "      <td>10.99</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>Human Resources Specialist</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9th week</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9974.0</td>\n",
       "      <td>32900.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70255675</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>BAT1586599</td>\n",
       "      <td>7.26</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>Driver</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9th week</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>38295.0</td>\n",
       "      <td>34900.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1893936</td>\n",
       "      <td>15000</td>\n",
       "      <td>15000</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>BAT4808022</td>\n",
       "      <td>19.72</td>\n",
       "      <td>D</td>\n",
       "      <td>D5</td>\n",
       "      <td>Us office of Personnel Management</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135th week</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55564.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7652106</td>\n",
       "      <td>16000</td>\n",
       "      <td>16000</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>BAT2833642</td>\n",
       "      <td>10.64</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>LAUSD-HOLLYWOOD HIGH SCHOOL</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96th week</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47159.0</td>\n",
       "      <td>47033.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   member_id  loan_amnt  funded_amnt  funded_amnt_inv       term  \\\n",
       "0   58189336      14350        14350          14350.0  36 months   \n",
       "1   70011223       4800         4800           4800.0  36 months   \n",
       "2   70255675      10000        10000          10000.0  36 months   \n",
       "3    1893936      15000        15000          15000.0  36 months   \n",
       "4    7652106      16000        16000          16000.0  36 months   \n",
       "\n",
       "  batch_enrolled  int_rate grade sub_grade                          emp_title  \\\n",
       "0                    19.19     E        E3                              clerk   \n",
       "1     BAT1586599     10.99     B        B4         Human Resources Specialist   \n",
       "2     BAT1586599      7.26     A        A4                             Driver   \n",
       "3     BAT4808022     19.72     D        D5  Us office of Personnel Management   \n",
       "4     BAT2833642     10.64     B        B2        LAUSD-HOLLYWOOD HIGH SCHOOL   \n",
       "\n",
       "   ... collections_12_mths_ex_med mths_since_last_major_derog  \\\n",
       "0  ...                        0.0                        74.0   \n",
       "1  ...                        0.0                         NaN   \n",
       "2  ...                        0.0                         NaN   \n",
       "3  ...                        0.0                         NaN   \n",
       "4  ...                        0.0                         NaN   \n",
       "\n",
       "   application_type verification_status_joint last_week_pay acc_now_delinq  \\\n",
       "0        INDIVIDUAL                       NaN     26th week            0.0   \n",
       "1        INDIVIDUAL                       NaN      9th week            0.0   \n",
       "2        INDIVIDUAL                       NaN      9th week            0.0   \n",
       "3        INDIVIDUAL                       NaN    135th week            0.0   \n",
       "4        INDIVIDUAL                       NaN     96th week            0.0   \n",
       "\n",
       "  tot_coll_amt tot_cur_bal total_rev_hi_lim loan_status  \n",
       "0          0.0     28699.0          30800.0           0  \n",
       "1          0.0      9974.0          32900.0           0  \n",
       "2         65.0     38295.0          34900.0           0  \n",
       "3          0.0     55564.0          24700.0           0  \n",
       "4          0.0     47159.0          47033.0           0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_last_delinq</th>\n",
       "      <th>...</th>\n",
       "      <th>total_rec_late_fee</th>\n",
       "      <th>recoveries</th>\n",
       "      <th>collection_recovery_fee</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>mths_since_last_major_derog</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>tot_coll_amt</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.324280e+05</td>\n",
       "      <td>532428.000</td>\n",
       "      <td>532428.000</td>\n",
       "      <td>532428.000</td>\n",
       "      <td>532428.000</td>\n",
       "      <td>532425.000</td>\n",
       "      <td>532428.000</td>\n",
       "      <td>532412.000</td>\n",
       "      <td>532412.000</td>\n",
       "      <td>259874.000</td>\n",
       "      <td>...</td>\n",
       "      <td>532428.000</td>\n",
       "      <td>532428.000</td>\n",
       "      <td>532428.000</td>\n",
       "      <td>532333.000</td>\n",
       "      <td>132980.000</td>\n",
       "      <td>532412.000</td>\n",
       "      <td>490424.000</td>\n",
       "      <td>490424.000</td>\n",
       "      <td>490424.000</td>\n",
       "      <td>532428.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.500547e+07</td>\n",
       "      <td>14757.596</td>\n",
       "      <td>14744.271</td>\n",
       "      <td>14704.927</td>\n",
       "      <td>13.243</td>\n",
       "      <td>75029.843</td>\n",
       "      <td>18.139</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.695</td>\n",
       "      <td>34.056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395</td>\n",
       "      <td>45.718</td>\n",
       "      <td>4.859</td>\n",
       "      <td>0.014</td>\n",
       "      <td>44.121</td>\n",
       "      <td>0.005</td>\n",
       "      <td>213.562</td>\n",
       "      <td>139554.111</td>\n",
       "      <td>32080.573</td>\n",
       "      <td>0.236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.412148e+07</td>\n",
       "      <td>8434.420</td>\n",
       "      <td>8429.139</td>\n",
       "      <td>8441.290</td>\n",
       "      <td>4.380</td>\n",
       "      <td>65199.845</td>\n",
       "      <td>8.369</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.997</td>\n",
       "      <td>21.885</td>\n",
       "      <td>...</td>\n",
       "      <td>4.092</td>\n",
       "      <td>409.647</td>\n",
       "      <td>63.123</td>\n",
       "      <td>0.133</td>\n",
       "      <td>22.198</td>\n",
       "      <td>0.079</td>\n",
       "      <td>1958.572</td>\n",
       "      <td>153914.877</td>\n",
       "      <td>38053.035</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.047300e+04</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.320</td>\n",
       "      <td>1200.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.086688e+07</td>\n",
       "      <td>8000.000</td>\n",
       "      <td>8000.000</td>\n",
       "      <td>8000.000</td>\n",
       "      <td>9.990</td>\n",
       "      <td>45000.000</td>\n",
       "      <td>11.930</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>29839.750</td>\n",
       "      <td>14000.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.709590e+07</td>\n",
       "      <td>13000.000</td>\n",
       "      <td>13000.000</td>\n",
       "      <td>13000.000</td>\n",
       "      <td>12.990</td>\n",
       "      <td>65000.000</td>\n",
       "      <td>17.650</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>31.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>44.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>80669.500</td>\n",
       "      <td>23700.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.848920e+07</td>\n",
       "      <td>20000.000</td>\n",
       "      <td>20000.000</td>\n",
       "      <td>20000.000</td>\n",
       "      <td>16.200</td>\n",
       "      <td>90000.000</td>\n",
       "      <td>23.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>61.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>208479.250</td>\n",
       "      <td>39800.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.354484e+07</td>\n",
       "      <td>35000.000</td>\n",
       "      <td>35000.000</td>\n",
       "      <td>35000.000</td>\n",
       "      <td>28.990</td>\n",
       "      <td>9500000.000</td>\n",
       "      <td>672.520</td>\n",
       "      <td>30.000</td>\n",
       "      <td>31.000</td>\n",
       "      <td>180.000</td>\n",
       "      <td>...</td>\n",
       "      <td>358.680</td>\n",
       "      <td>33520.270</td>\n",
       "      <td>7002.190</td>\n",
       "      <td>16.000</td>\n",
       "      <td>180.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>496651.000</td>\n",
       "      <td>8000078.000</td>\n",
       "      <td>9999999.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          member_id   loan_amnt  funded_amnt  funded_amnt_inv    int_rate  \\\n",
       "count  5.324280e+05  532428.000   532428.000       532428.000  532428.000   \n",
       "mean   3.500547e+07   14757.596    14744.271        14704.927      13.243   \n",
       "std    2.412148e+07    8434.420     8429.139         8441.290       4.380   \n",
       "min    7.047300e+04     500.000      500.000            0.000       5.320   \n",
       "25%    1.086688e+07    8000.000     8000.000         8000.000       9.990   \n",
       "50%    3.709590e+07   13000.000    13000.000        13000.000      12.990   \n",
       "75%    5.848920e+07   20000.000    20000.000        20000.000      16.200   \n",
       "max    7.354484e+07   35000.000    35000.000        35000.000      28.990   \n",
       "\n",
       "        annual_inc         dti  delinq_2yrs  inq_last_6mths  \\\n",
       "count   532425.000  532428.000   532412.000      532412.000   \n",
       "mean     75029.843      18.139        0.314           0.695   \n",
       "std      65199.845       8.369        0.860           0.997   \n",
       "min       1200.000       0.000        0.000           0.000   \n",
       "25%      45000.000      11.930        0.000           0.000   \n",
       "50%      65000.000      17.650        0.000           0.000   \n",
       "75%      90000.000      23.950        0.000           1.000   \n",
       "max    9500000.000     672.520       30.000          31.000   \n",
       "\n",
       "       mths_since_last_delinq  ...  total_rec_late_fee  recoveries  \\\n",
       "count              259874.000  ...          532428.000  532428.000   \n",
       "mean                   34.056  ...               0.395      45.718   \n",
       "std                    21.885  ...               4.092     409.647   \n",
       "min                     0.000  ...               0.000       0.000   \n",
       "25%                    15.000  ...               0.000       0.000   \n",
       "50%                    31.000  ...               0.000       0.000   \n",
       "75%                    50.000  ...               0.000       0.000   \n",
       "max                   180.000  ...             358.680   33520.270   \n",
       "\n",
       "       collection_recovery_fee  collections_12_mths_ex_med  \\\n",
       "count               532428.000                  532333.000   \n",
       "mean                     4.859                       0.014   \n",
       "std                     63.123                       0.133   \n",
       "min                      0.000                       0.000   \n",
       "25%                      0.000                       0.000   \n",
       "50%                      0.000                       0.000   \n",
       "75%                      0.000                       0.000   \n",
       "max                   7002.190                      16.000   \n",
       "\n",
       "       mths_since_last_major_derog  acc_now_delinq  tot_coll_amt  tot_cur_bal  \\\n",
       "count                   132980.000      532412.000    490424.000   490424.000   \n",
       "mean                        44.121           0.005       213.562   139554.111   \n",
       "std                         22.198           0.079      1958.572   153914.877   \n",
       "min                          0.000           0.000         0.000        0.000   \n",
       "25%                         27.000           0.000         0.000    29839.750   \n",
       "50%                         44.000           0.000         0.000    80669.500   \n",
       "75%                         61.000           0.000         0.000   208479.250   \n",
       "max                        180.000          14.000    496651.000  8000078.000   \n",
       "\n",
       "       total_rev_hi_lim  loan_status  \n",
       "count        490424.000   532428.000  \n",
       "mean          32080.573        0.236  \n",
       "std           38053.035        0.425  \n",
       "min               0.000        0.000  \n",
       "25%           14000.000        0.000  \n",
       "50%           23700.000        0.000  \n",
       "75%           39800.000        0.000  \n",
       "max         9999999.000        1.000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term',\n",
       "       'batch_enrolled', 'int_rate', 'grade', 'sub_grade', 'emp_title',\n",
       "       'emp_length', 'home_ownership', 'annual_inc', 'verification_status',\n",
       "       'pymnt_plan', 'desc', 'purpose', 'title', 'zip_code', 'addr_state',\n",
       "       'dti', 'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_delinq',\n",
       "       'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n",
       "       'revol_util', 'total_acc', 'initial_list_status', 'total_rec_int',\n",
       "       'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
       "       'collections_12_mths_ex_med', 'mths_since_last_major_derog',\n",
       "       'application_type', 'verification_status_joint', 'last_week_pay',\n",
       "       'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim',\n",
       "       'loan_status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces months string with '' \n",
    "df['term'].replace(to_replace=' months', value='', regex=True, inplace=True) \n",
    "\n",
    "# Convert it to numeric\n",
    "df['term'] = pd.to_numeric(df['term'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36    372793\n",
       "60    159635\n",
       "Name: term, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['term'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_length'].replace('n/a', '0', inplace=True)\n",
    "df['emp_length'].replace(to_replace='\\+ years', value='', regex=True, inplace=True)\n",
    "df['emp_length'].replace(to_replace=' years', value='', regex=True, inplace=True)\n",
    "df['emp_length'].replace(to_replace='< 1 year', value='0', regex=True, inplace=True)\n",
    "df['emp_length'].replace(to_replace=' year', value='', regex=True, inplace=True)\n",
    "\n",
    "# Convert it to numeric\n",
    "df['emp_length'] = pd.to_numeric(df['emp_length'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0    175105\n",
       "2.0      47276\n",
       "0.0      42253\n",
       "3.0      42175\n",
       "1.0      34202\n",
       "5.0      33393\n",
       "4.0      31581\n",
       "7.0      26680\n",
       "8.0      26443\n",
       "6.0      25741\n",
       "9.0      20688\n",
       "Name: emp_length, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emp_length'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sub_grade'].replace(to_replace='A', value='0', regex=True, inplace=True)\n",
    "df['sub_grade'].replace(to_replace='B', value='1', regex=True, inplace=True)\n",
    "df['sub_grade'].replace(to_replace='C', value='2', regex=True, inplace=True)\n",
    "df['sub_grade'].replace(to_replace='D', value='3', regex=True, inplace=True)\n",
    "df['sub_grade'].replace(to_replace='E', value='4', regex=True, inplace=True)\n",
    "df['sub_grade'].replace(to_replace='F', value='5', regex=True, inplace=True)\n",
    "df['sub_grade'].replace(to_replace='G', value='6', regex=True, inplace=True) \n",
    "\n",
    "# Convert it to numeric\n",
    "df['sub_grade'] = pd.to_numeric(df['sub_grade'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13    33844\n",
       "14    33198\n",
       "21    31975\n",
       "22    31356\n",
       "23    30080\n",
       "12    29390\n",
       "15    29313\n",
       "24    29103\n",
       "5     27016\n",
       "11    26968\n",
       "25    24985\n",
       "31    21712\n",
       "4     20823\n",
       "32    17991\n",
       "33    15771\n",
       "34    15226\n",
       "3     14082\n",
       "1     13653\n",
       "2     13533\n",
       "35    12867\n",
       "41    10928\n",
       "42    10255\n",
       "43     8488\n",
       "44     7051\n",
       "45     5773\n",
       "51     4350\n",
       "52     3196\n",
       "53     2708\n",
       "54     2056\n",
       "55     1516\n",
       "61     1112\n",
       "62      824\n",
       "63      559\n",
       "64      391\n",
       "65      335\n",
       "Name: sub_grade, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sub_grade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_week_pay'].replace(to_replace='th week', value='', regex=True, inplace=True)\n",
    "\n",
    "df['last_week_pay'].replace(to_replace='NA', value='', regex=True, inplace=True)\n",
    "\n",
    "# Convert it to numeric\n",
    "df['last_week_pay'] = pd.to_numeric(df['last_week_pay'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "member_id                           0\n",
       "loan_amnt                           0\n",
       "funded_amnt                         0\n",
       "funded_amnt_inv                     0\n",
       "term                                0\n",
       "batch_enrolled                  85149\n",
       "int_rate                            0\n",
       "grade                               0\n",
       "sub_grade                           0\n",
       "emp_title                       30833\n",
       "emp_length                      26891\n",
       "home_ownership                      0\n",
       "annual_inc                          3\n",
       "verification_status                 0\n",
       "pymnt_plan                          0\n",
       "desc                           456829\n",
       "purpose                             0\n",
       "title                              90\n",
       "zip_code                            0\n",
       "addr_state                          0\n",
       "dti                                 0\n",
       "delinq_2yrs                        16\n",
       "inq_last_6mths                     16\n",
       "mths_since_last_delinq         272554\n",
       "mths_since_last_record         450305\n",
       "open_acc                           16\n",
       "pub_rec                            16\n",
       "revol_bal                           0\n",
       "revol_util                        287\n",
       "total_acc                          16\n",
       "initial_list_status                 0\n",
       "total_rec_int                       0\n",
       "total_rec_late_fee                  0\n",
       "recoveries                          0\n",
       "collection_recovery_fee             0\n",
       "collections_12_mths_ex_med         95\n",
       "mths_since_last_major_derog    399448\n",
       "application_type                    0\n",
       "verification_status_joint      532123\n",
       "last_week_pay                   10614\n",
       "acc_now_delinq                     16\n",
       "tot_coll_amt                    42004\n",
       "tot_cur_bal                     42004\n",
       "total_rev_hi_lim                42004\n",
       "loan_status                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['term', 'loan_amnt', 'funded_amnt', 'last_week_pay', 'int_rate', 'sub_grade',\n",
    "           'annual_inc', 'dti', 'mths_since_last_delinq', 'mths_since_last_record', 'open_acc',\n",
    "           'revol_bal', 'revol_util', 'total_acc', 'total_rec_int', 'mths_since_last_major_derog', \n",
    "           'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim', 'emp_length']\n",
    "for col in columns:\n",
    "    df[col].fillna(df[col].median(), inplace=True)  # Filling NaN values with median of each column present in columns.\n",
    "\n",
    "  \n",
    "num_cols = ['acc_now_delinq', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'collections_12_mths_ex_med']\n",
    "for col in num_cols:\n",
    "    df[col].fillna(0, inplace=True)        ## Filling NaN values with 0 for each column present in columns.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts categorical variable into dummy/indicator variables.\n",
    "cate_attr = ['home_ownership', 'purpose']\n",
    "for cat in cate_attr:\n",
    "   df_col = [cat]\n",
    "   df[cat] = df[cat].astype(\"category\")\n",
    "   df[cat] = pd.get_dummies(df, columns=df_col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df.copy()\n",
    "final_df = final_df.drop(['loan_status'], axis=1)\n",
    "data=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(data)*0.8)\n",
    "\n",
    "\n",
    "major = ['tot_cur_bal', 'last_week_pay', 'total_rev_hi_lim', 'int_rate']\n",
    "\n",
    "\n",
    "minor = ['tot_cur_bal','zip_code', 'addr_state', 'revol_util', 'revol_bal', 'sub_grade', 'annual_inc', 'total_rec_int']\n",
    "\n",
    "#data = data.dropna()\n",
    "X_train = data[data['loan_status'] >= 0].iloc[:split,:-1][major]\n",
    "Y_train = data[data['loan_status'] >= 0].iloc[:split,-1:]\n",
    "X_test = data[data['loan_status'] >= 0].iloc[split:,:-1][major]\n",
    "Y_test = data[data['loan_status'] >= 0].iloc[split:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>last_week_pay</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>int_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28699.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30800.0</td>\n",
       "      <td>19.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9974.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>32900.0</td>\n",
       "      <td>10.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38295.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>34900.0</td>\n",
       "      <td>7.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55564.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>19.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47159.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>47033.0</td>\n",
       "      <td>10.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tot_cur_bal  last_week_pay  total_rev_hi_lim  int_rate\n",
       "0      28699.0           26.0           30800.0     19.19\n",
       "1       9974.0            9.0           32900.0     10.99\n",
       "2      38295.0            9.0           34900.0      7.26\n",
       "3      55564.0          135.0           24700.0     19.72\n",
       "4      47159.0           96.0           47033.0     10.64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-3db9857bd095>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, Y_train)\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100building tree 13 of 100\n",
      "\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   25.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   43.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1, verbose=5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, verbose=5, n_jobs=-1)\n",
    "rf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    2.1s finished\n"
     ]
    }
   ],
   "source": [
    "preds = rf.predict(X_test)\n",
    "fpr1, tpr1, thr1 = roc_curve(Y_test['loan_status'], preds)\n",
    "auc1 = roc_auc_score(Y_test['loan_status'], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647088251669928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8650057284525665"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(auc1)\n",
    "rf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr1,tpr1)\n",
    "plt.plot(fpr1,fpr1)\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tot_cur_bal</td>\n",
       "      <td>0.272680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>0.272607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_rev_hi_lim</td>\n",
       "      <td>0.236367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>last_week_pay</td>\n",
       "      <td>0.218347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Feature  Importance\n",
       "0       tot_cur_bal    0.272680\n",
       "1          int_rate    0.272607\n",
       "2  total_rev_hi_lim    0.236367\n",
       "3     last_week_pay    0.218347"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = list(zip(X_train.columns.values, rf.feature_importances_))\n",
    "fi = sorted(fi, key=lambda x: -x[1])\n",
    "pd.DataFrame(fi, columns=[\"Feature\",\"Importance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parv/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass param_name=n_estimators, param_range=[100 200 300 400 500 600 700 800 900] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "#rfc_train_scores, rfc_valid_scores = validation_curve(RandomForestClassifier(oob_score=True), X_train, Y_train.values.reshape(-1),'n_estimators', np.arange(100,1000,100), n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-e6b0c83550eb>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  knc.fit(X_train, Y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_jobs=-1, n_neighbors=150)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knc = KNeighborsClassifier(n_neighbors=150, n_jobs=-1)\n",
    "knc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_knc = knc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8116747741487144"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knc_train_scores, knc_valid_scores = validation_curve(KNeighborsClassifier(), X_train, Y_train.values.reshape(-1),'n_neighbors', np.arange(50,300,50), n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parv/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 234509.02, NNZs: 4, Bias: 4676.148849, T: 425942, Avg. loss: 17110461942.327681\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 223958.43, NNZs: 4, Bias: 5064.878742, T: 851884, Avg. loss: 1853550749.090038\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 216328.81, NNZs: 4, Bias: 5267.257137, T: 1277826, Avg. loss: 1118131535.501759\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 206727.11, NNZs: 4, Bias: 5348.981098, T: 1703768, Avg. loss: 764153750.152372\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 199479.97, NNZs: 4, Bias: 5405.134853, T: 2129710, Avg. loss: 599313768.391783\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 192411.42, NNZs: 4, Bias: 5425.764044, T: 2555652, Avg. loss: 481162839.200647\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 185664.39, NNZs: 4, Bias: 5432.872803, T: 2981594, Avg. loss: 408167926.127279\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 179231.40, NNZs: 4, Bias: 5428.941417, T: 3407536, Avg. loss: 354616013.231250\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 173218.31, NNZs: 4, Bias: 5416.416260, T: 3833478, Avg. loss: 308560847.462855\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 167650.15, NNZs: 4, Bias: 5396.790614, T: 4259420, Avg. loss: 279337544.700323\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 162577.79, NNZs: 4, Bias: 5377.740063, T: 4685362, Avg. loss: 252001262.267890\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 157369.61, NNZs: 4, Bias: 5352.322297, T: 5111304, Avg. loss: 227383932.935455\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 152472.77, NNZs: 4, Bias: 5321.092613, T: 5537246, Avg. loss: 213032407.586246\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 147917.10, NNZs: 4, Bias: 5292.237375, T: 5963188, Avg. loss: 193660239.690869\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 143551.34, NNZs: 4, Bias: 5263.296924, T: 6389130, Avg. loss: 184824524.340883\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 139536.82, NNZs: 4, Bias: 5233.661707, T: 6815072, Avg. loss: 168683721.621795\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 135920.53, NNZs: 4, Bias: 5205.571554, T: 7241014, Avg. loss: 161257142.306553\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 131939.21, NNZs: 4, Bias: 5172.236261, T: 7666956, Avg. loss: 149417652.269587\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 128277.03, NNZs: 4, Bias: 5141.524373, T: 8092898, Avg. loss: 142256109.271673\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 124931.77, NNZs: 4, Bias: 5111.168333, T: 8518840, Avg. loss: 136091347.005110\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 121692.30, NNZs: 4, Bias: 5080.969758, T: 8944782, Avg. loss: 127732294.406053\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 118822.50, NNZs: 4, Bias: 5052.727210, T: 9370724, Avg. loss: 119955560.125801\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 115989.82, NNZs: 4, Bias: 5024.075082, T: 9796666, Avg. loss: 119735039.740462\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 113047.88, NNZs: 4, Bias: 4994.092454, T: 10222608, Avg. loss: 108762375.197846\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 110442.20, NNZs: 4, Bias: 4966.589288, T: 10648550, Avg. loss: 107295348.509078\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 108070.38, NNZs: 4, Bias: 4940.537268, T: 11074492, Avg. loss: 104011553.658028\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 105682.11, NNZs: 4, Bias: 4913.713038, T: 11500434, Avg. loss: 99000647.109349\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 103459.67, NNZs: 4, Bias: 4888.736784, T: 11926376, Avg. loss: 95301635.925090\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 101053.54, NNZs: 4, Bias: 4861.323978, T: 12352318, Avg. loss: 90594309.698975\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 98877.29, NNZs: 4, Bias: 4836.130860, T: 12778260, Avg. loss: 87890236.246793\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 96666.41, NNZs: 4, Bias: 4809.453876, T: 13204202, Avg. loss: 85012162.455071\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 94661.15, NNZs: 4, Bias: 4785.680370, T: 13630144, Avg. loss: 83267154.331627\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 92778.51, NNZs: 4, Bias: 4761.311626, T: 14056086, Avg. loss: 81720867.542296\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 90819.15, NNZs: 4, Bias: 4736.734338, T: 14482028, Avg. loss: 77735404.960647\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 88978.00, NNZs: 4, Bias: 4712.996709, T: 14907970, Avg. loss: 74597053.964605\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 87231.54, NNZs: 4, Bias: 4688.817333, T: 15333912, Avg. loss: 73433630.288987\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 85525.62, NNZs: 4, Bias: 4665.614034, T: 15759854, Avg. loss: 71005117.182222\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 83923.83, NNZs: 4, Bias: 4643.238741, T: 16185796, Avg. loss: 69541141.391229\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 82370.30, NNZs: 4, Bias: 4621.085869, T: 16611738, Avg. loss: 67565133.992564\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 80717.40, NNZs: 4, Bias: 4597.866683, T: 17037680, Avg. loss: 65257233.977262\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 79186.40, NNZs: 4, Bias: 4576.509801, T: 17463622, Avg. loss: 63582444.689186\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 77657.10, NNZs: 4, Bias: 4553.994265, T: 17889564, Avg. loss: 62205572.116586\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 76255.67, NNZs: 4, Bias: 4532.709602, T: 18315506, Avg. loss: 61308015.872334\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 74946.35, NNZs: 4, Bias: 4512.521640, T: 18741448, Avg. loss: 59720366.281706\n",
      "Total training time: 2.28 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 73611.04, NNZs: 4, Bias: 4491.841581, T: 19167390, Avg. loss: 58646662.752080\n",
      "Total training time: 2.33 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 72386.04, NNZs: 4, Bias: 4472.147986, T: 19593332, Avg. loss: 57418737.354620\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 71123.46, NNZs: 4, Bias: 4452.448467, T: 20019274, Avg. loss: 55622865.770122\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 69884.94, NNZs: 4, Bias: 4432.561202, T: 20445216, Avg. loss: 55291463.350787\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 68742.82, NNZs: 4, Bias: 4414.090756, T: 20871158, Avg. loss: 54060165.251710\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 67707.38, NNZs: 4, Bias: 4396.072498, T: 21297100, Avg. loss: 52459930.968506\n",
      "Total training time: 2.58 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 66675.21, NNZs: 4, Bias: 4378.628899, T: 21723042, Avg. loss: 52134067.168694\n",
      "Total training time: 2.63 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 65621.07, NNZs: 4, Bias: 4360.577515, T: 22148984, Avg. loss: 50216530.737844\n",
      "Total training time: 2.68 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 64581.19, NNZs: 4, Bias: 4342.514575, T: 22574926, Avg. loss: 48706631.208478\n",
      "Total training time: 2.73 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 63553.12, NNZs: 4, Bias: 4324.704251, T: 23000868, Avg. loss: 48246645.685606\n",
      "Total training time: 2.81 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 62631.17, NNZs: 4, Bias: 4308.025485, T: 23426810, Avg. loss: 47913578.375304\n",
      "Total training time: 2.90 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 61680.51, NNZs: 4, Bias: 4290.792902, T: 23852752, Avg. loss: 47040535.379878\n",
      "Total training time: 2.97 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 60782.41, NNZs: 4, Bias: 4273.886943, T: 24278694, Avg. loss: 45411254.704638\n",
      "Total training time: 3.04 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 59943.51, NNZs: 4, Bias: 4258.399634, T: 24704636, Avg. loss: 45806731.844075\n",
      "Total training time: 3.10 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 59150.80, NNZs: 4, Bias: 4242.912439, T: 25130578, Avg. loss: 45254583.621983\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 58336.06, NNZs: 4, Bias: 4227.608657, T: 25556520, Avg. loss: 44123276.023367\n",
      "Total training time: 3.20 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 57660.43, NNZs: 4, Bias: 4213.399463, T: 25982462, Avg. loss: 44091266.789581\n",
      "Total training time: 3.25 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 56913.22, NNZs: 4, Bias: 4198.690704, T: 26408404, Avg. loss: 42345690.364828\n",
      "Total training time: 3.31 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 56148.90, NNZs: 4, Bias: 4183.894738, T: 26834346, Avg. loss: 41606074.856154\n",
      "Total training time: 3.35 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 55430.95, NNZs: 4, Bias: 4169.826959, T: 27260288, Avg. loss: 41521989.111406\n",
      "Total training time: 3.41 seconds.\n",
      "-- Epoch 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 54685.55, NNZs: 4, Bias: 4154.943188, T: 27686230, Avg. loss: 40836460.727513\n",
      "Total training time: 3.46 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 53982.54, NNZs: 4, Bias: 4140.604804, T: 28112172, Avg. loss: 39840002.925063\n",
      "Total training time: 3.51 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 53315.01, NNZs: 4, Bias: 4126.928612, T: 28538114, Avg. loss: 39648006.608472\n",
      "Total training time: 3.57 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 52629.34, NNZs: 4, Bias: 4112.448814, T: 28964056, Avg. loss: 37790209.670763\n",
      "Total training time: 3.61 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 51960.70, NNZs: 4, Bias: 4098.299023, T: 29389998, Avg. loss: 37528986.850778\n",
      "Total training time: 3.66 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 51317.30, NNZs: 4, Bias: 4084.792255, T: 29815940, Avg. loss: 37051927.919712\n",
      "Total training time: 3.72 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 50732.08, NNZs: 4, Bias: 4071.537279, T: 30241882, Avg. loss: 36766612.918558\n",
      "Total training time: 3.77 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 50100.63, NNZs: 4, Bias: 4057.813651, T: 30667824, Avg. loss: 35766725.424463\n",
      "Total training time: 3.81 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 49512.84, NNZs: 4, Bias: 4044.530915, T: 31093766, Avg. loss: 35539257.385491\n",
      "Total training time: 3.86 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 48904.72, NNZs: 4, Bias: 4031.349156, T: 31519708, Avg. loss: 35239614.895143\n",
      "Total training time: 3.91 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 48376.91, NNZs: 4, Bias: 4019.006945, T: 31945650, Avg. loss: 35870612.187571\n",
      "Total training time: 3.97 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 47865.30, NNZs: 4, Bias: 4006.558274, T: 32371592, Avg. loss: 34460934.023090\n",
      "Total training time: 4.02 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 47322.54, NNZs: 4, Bias: 3993.910587, T: 32797534, Avg. loss: 33759650.035567\n",
      "Total training time: 4.07 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 46800.57, NNZs: 4, Bias: 3981.444749, T: 33223476, Avg. loss: 33448088.323487\n",
      "Total training time: 4.12 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 46321.45, NNZs: 4, Bias: 3969.541994, T: 33649418, Avg. loss: 33898723.145401\n",
      "Total training time: 4.18 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 45827.58, NNZs: 4, Bias: 3957.866621, T: 34075360, Avg. loss: 33407222.536957\n",
      "Total training time: 4.24 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 45355.05, NNZs: 4, Bias: 3946.041743, T: 34501302, Avg. loss: 32377686.005806\n",
      "Total training time: 4.30 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 44893.45, NNZs: 4, Bias: 3934.935716, T: 34927244, Avg. loss: 31977202.502074\n",
      "Total training time: 4.37 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 44448.51, NNZs: 4, Bias: 3923.698746, T: 35353186, Avg. loss: 31037458.528184\n",
      "Total training time: 4.43 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 43976.10, NNZs: 4, Bias: 3912.292952, T: 35779128, Avg. loss: 31080408.881862\n",
      "Total training time: 4.49 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 43544.06, NNZs: 4, Bias: 3901.241942, T: 36205070, Avg. loss: 30875737.328480\n",
      "Total training time: 4.56 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 43136.83, NNZs: 4, Bias: 3890.605137, T: 36631012, Avg. loss: 31040046.299502\n",
      "Total training time: 4.62 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 42737.63, NNZs: 4, Bias: 3879.987079, T: 37056954, Avg. loss: 30068148.285621\n",
      "Total training time: 4.67 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 42287.55, NNZs: 4, Bias: 3868.527132, T: 37482896, Avg. loss: 29455881.680899\n",
      "Total training time: 4.71 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 41869.99, NNZs: 4, Bias: 3857.731310, T: 37908838, Avg. loss: 29514848.756342\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 41506.71, NNZs: 4, Bias: 3847.675403, T: 38334780, Avg. loss: 29263494.972761\n",
      "Total training time: 4.83 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 41190.23, NNZs: 4, Bias: 3837.995430, T: 38760722, Avg. loss: 29056521.416581\n",
      "Total training time: 4.88 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 40853.17, NNZs: 4, Bias: 3827.940001, T: 39186664, Avg. loss: 28123571.746323\n",
      "Total training time: 4.93 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 40479.85, NNZs: 4, Bias: 3817.752476, T: 39612606, Avg. loss: 28095649.468016\n",
      "Total training time: 4.98 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 40156.99, NNZs: 4, Bias: 3808.160925, T: 40038548, Avg. loss: 28256453.285587\n",
      "Total training time: 5.03 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 39798.41, NNZs: 4, Bias: 3798.048606, T: 40464490, Avg. loss: 27636840.919884\n",
      "Total training time: 5.08 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 39474.94, NNZs: 4, Bias: 3788.327008, T: 40890432, Avg. loss: 27054316.625563\n",
      "Total training time: 5.13 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 39131.52, NNZs: 4, Bias: 3778.741347, T: 41316374, Avg. loss: 27037007.114169\n",
      "Total training time: 5.18 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 38805.84, NNZs: 4, Bias: 3769.569777, T: 41742316, Avg. loss: 26674035.206509\n",
      "Total training time: 5.22 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 38485.42, NNZs: 4, Bias: 3759.948568, T: 42168258, Avg. loss: 26427194.032489\n",
      "Total training time: 5.28 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 38148.18, NNZs: 4, Bias: 3750.286597, T: 42594200, Avg. loss: 25898857.658118\n",
      "Total training time: 5.35 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 37847.08, NNZs: 4, Bias: 3741.158729, T: 43020142, Avg. loss: 25750685.080685\n",
      "Total training time: 5.42 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 37547.42, NNZs: 4, Bias: 3731.838204, T: 43446084, Avg. loss: 25724396.896949\n",
      "Total training time: 5.49 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 37253.59, NNZs: 4, Bias: 3722.655282, T: 43872026, Avg. loss: 25540376.459405\n",
      "Total training time: 5.56 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 36966.08, NNZs: 4, Bias: 3713.673549, T: 44297968, Avg. loss: 25065334.107071\n",
      "Total training time: 5.62 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 36703.94, NNZs: 4, Bias: 3705.153206, T: 44723910, Avg. loss: 24662569.229236\n",
      "Total training time: 5.67 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 36463.52, NNZs: 4, Bias: 3696.754116, T: 45149852, Avg. loss: 24524004.472574\n",
      "Total training time: 5.72 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 36149.90, NNZs: 4, Bias: 3687.573896, T: 45575794, Avg. loss: 24009109.759284\n",
      "Total training time: 5.79 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 35912.00, NNZs: 4, Bias: 3679.163804, T: 46001736, Avg. loss: 24950774.981388\n",
      "Total training time: 5.85 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 35645.43, NNZs: 4, Bias: 3670.596079, T: 46427678, Avg. loss: 23809626.746807\n",
      "Total training time: 5.90 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 35376.77, NNZs: 4, Bias: 3662.079928, T: 46853620, Avg. loss: 23422225.173638\n",
      "Total training time: 5.96 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 35110.67, NNZs: 4, Bias: 3653.625047, T: 47279562, Avg. loss: 23188736.479010\n",
      "Total training time: 6.03 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 34874.96, NNZs: 4, Bias: 3645.473359, T: 47705504, Avg. loss: 23407171.865967\n",
      "Total training time: 6.11 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 34622.02, NNZs: 4, Bias: 3637.193317, T: 48131446, Avg. loss: 23015237.277825\n",
      "Total training time: 6.17 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 34369.42, NNZs: 4, Bias: 3628.849883, T: 48557388, Avg. loss: 22939623.795592\n",
      "Total training time: 6.23 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 34143.65, NNZs: 4, Bias: 3620.780278, T: 48983330, Avg. loss: 23004933.060182\n",
      "Total training time: 6.28 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 33930.63, NNZs: 4, Bias: 3613.016067, T: 49409272, Avg. loss: 22423224.012296\n",
      "Total training time: 6.33 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 33709.30, NNZs: 4, Bias: 3605.009297, T: 49835214, Avg. loss: 22297259.737182\n",
      "Total training time: 6.37 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 33495.47, NNZs: 4, Bias: 3597.239892, T: 50261156, Avg. loss: 22042782.398073\n",
      "Total training time: 6.42 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 33261.41, NNZs: 4, Bias: 3589.145198, T: 50687098, Avg. loss: 21634471.544961\n",
      "Total training time: 6.47 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 33051.45, NNZs: 4, Bias: 3581.622739, T: 51113040, Avg. loss: 21922691.970116\n",
      "Total training time: 6.53 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 32835.69, NNZs: 4, Bias: 3573.812567, T: 51538982, Avg. loss: 21233040.388321\n",
      "Total training time: 6.60 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 32628.42, NNZs: 4, Bias: 3566.379450, T: 51964924, Avg. loss: 21395122.724776\n",
      "Total training time: 6.67 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 32436.89, NNZs: 4, Bias: 3559.079877, T: 52390866, Avg. loss: 21161171.655526\n",
      "Total training time: 6.74 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 32238.92, NNZs: 4, Bias: 3551.748184, T: 52816808, Avg. loss: 21039711.359343\n",
      "Total training time: 6.81 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 32031.95, NNZs: 4, Bias: 3544.415669, T: 53242750, Avg. loss: 20785973.363564\n",
      "Total training time: 6.86 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 31836.36, NNZs: 4, Bias: 3536.973205, T: 53668692, Avg. loss: 20673529.575954\n",
      "Total training time: 6.92 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 31653.88, NNZs: 4, Bias: 3530.233478, T: 54094634, Avg. loss: 20877271.302561\n",
      "Total training time: 6.99 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 31472.08, NNZs: 4, Bias: 3523.131317, T: 54520576, Avg. loss: 20300082.913992\n",
      "Total training time: 7.05 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 31295.02, NNZs: 4, Bias: 3516.325632, T: 54946518, Avg. loss: 20295160.250175\n",
      "Total training time: 7.12 seconds.\n",
      "-- Epoch 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 31122.64, NNZs: 4, Bias: 3509.653788, T: 55372460, Avg. loss: 20031597.024487\n",
      "Total training time: 7.20 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 30948.01, NNZs: 4, Bias: 3502.870208, T: 55798402, Avg. loss: 19767559.965592\n",
      "Total training time: 7.26 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 30807.25, NNZs: 4, Bias: 3496.493986, T: 56224344, Avg. loss: 20214139.380895\n",
      "Total training time: 7.31 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 30613.33, NNZs: 4, Bias: 3489.293286, T: 56650286, Avg. loss: 19639806.387683\n",
      "Total training time: 7.36 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 30422.55, NNZs: 4, Bias: 3482.173535, T: 57076228, Avg. loss: 19472928.748536\n",
      "Total training time: 7.40 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 30255.75, NNZs: 4, Bias: 3475.405350, T: 57502170, Avg. loss: 19302200.242265\n",
      "Total training time: 7.46 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 30054.44, NNZs: 4, Bias: 3468.440885, T: 57928112, Avg. loss: 19119803.091930\n",
      "Total training time: 7.50 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 29889.14, NNZs: 4, Bias: 3461.966145, T: 58354054, Avg. loss: 18740070.784383\n",
      "Total training time: 7.55 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 29718.42, NNZs: 4, Bias: 3455.325309, T: 58779996, Avg. loss: 18872098.693566\n",
      "Total training time: 7.61 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 29557.68, NNZs: 4, Bias: 3448.658865, T: 59205938, Avg. loss: 18650173.360038\n",
      "Total training time: 7.68 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 29444.91, NNZs: 4, Bias: 3442.552534, T: 59631880, Avg. loss: 19062892.036763\n",
      "Total training time: 7.76 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 29312.18, NNZs: 4, Bias: 3436.479440, T: 60057822, Avg. loss: 18765072.630647\n",
      "Total training time: 7.85 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 29164.13, NNZs: 4, Bias: 3430.157205, T: 60483764, Avg. loss: 18421684.036047\n",
      "Total training time: 7.91 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 29017.45, NNZs: 4, Bias: 3423.802543, T: 60909706, Avg. loss: 18245233.067436\n",
      "Total training time: 7.97 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 28918.73, NNZs: 4, Bias: 3418.097526, T: 61335648, Avg. loss: 18614802.424869\n",
      "Total training time: 8.03 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 28766.62, NNZs: 4, Bias: 3411.840835, T: 61761590, Avg. loss: 18038578.109652\n",
      "Total training time: 8.08 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 28640.16, NNZs: 4, Bias: 3405.889584, T: 62187532, Avg. loss: 17923156.517135\n",
      "Total training time: 8.12 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 28488.76, NNZs: 4, Bias: 3399.777928, T: 62613474, Avg. loss: 17550993.355796\n",
      "Total training time: 8.17 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 28339.72, NNZs: 4, Bias: 3393.670177, T: 63039416, Avg. loss: 17982358.881538\n",
      "Total training time: 8.22 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 28195.68, NNZs: 4, Bias: 3387.460137, T: 63465358, Avg. loss: 17309609.931375\n",
      "Total training time: 8.27 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 28065.11, NNZs: 4, Bias: 3381.597686, T: 63891300, Avg. loss: 17386884.195835\n",
      "Total training time: 8.34 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 27941.61, NNZs: 4, Bias: 3375.877469, T: 64317242, Avg. loss: 17208665.978921\n",
      "Total training time: 8.41 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 27802.23, NNZs: 4, Bias: 3370.063779, T: 64743184, Avg. loss: 16935447.176757\n",
      "Total training time: 8.47 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 27706.80, NNZs: 4, Bias: 3364.654735, T: 65169126, Avg. loss: 17457812.939009\n",
      "Total training time: 8.52 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 27574.80, NNZs: 4, Bias: 3358.761766, T: 65595068, Avg. loss: 16880843.155239\n",
      "Total training time: 8.57 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 27461.07, NNZs: 4, Bias: 3353.172027, T: 66021010, Avg. loss: 16795349.044529\n",
      "Total training time: 8.62 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 27332.41, NNZs: 4, Bias: 3347.629430, T: 66446952, Avg. loss: 16712398.969801\n",
      "Total training time: 8.67 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 27205.64, NNZs: 4, Bias: 3341.919667, T: 66872894, Avg. loss: 16520929.009383\n",
      "Total training time: 8.71 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 27107.33, NNZs: 4, Bias: 3336.497733, T: 67298836, Avg. loss: 16843778.705670\n",
      "Total training time: 8.76 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 26998.72, NNZs: 4, Bias: 3331.123462, T: 67724778, Avg. loss: 16180587.593303\n",
      "Total training time: 8.81 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 26867.45, NNZs: 4, Bias: 3325.452189, T: 68150720, Avg. loss: 16105062.013298\n",
      "Total training time: 8.86 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 26752.00, NNZs: 4, Bias: 3320.069212, T: 68576662, Avg. loss: 16162600.927250\n",
      "Total training time: 8.91 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 26638.60, NNZs: 4, Bias: 3314.591596, T: 69002604, Avg. loss: 16177797.223433\n",
      "Total training time: 8.96 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 26518.90, NNZs: 4, Bias: 3309.123847, T: 69428546, Avg. loss: 15773010.908465\n",
      "Total training time: 9.02 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 26422.49, NNZs: 4, Bias: 3303.939153, T: 69854488, Avg. loss: 15889975.966884\n",
      "Total training time: 9.08 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 26317.08, NNZs: 4, Bias: 3298.649469, T: 70280430, Avg. loss: 15907960.137102\n",
      "Total training time: 9.14 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 26206.58, NNZs: 4, Bias: 3293.287846, T: 70706372, Avg. loss: 15650278.796452\n",
      "Total training time: 9.20 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 26102.24, NNZs: 4, Bias: 3288.126520, T: 71132314, Avg. loss: 15599671.140042\n",
      "Total training time: 9.27 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 25983.39, NNZs: 4, Bias: 3282.738246, T: 71558256, Avg. loss: 15394870.972059\n",
      "Total training time: 9.34 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 25887.34, NNZs: 4, Bias: 3277.616795, T: 71984198, Avg. loss: 15459751.631722\n",
      "Total training time: 9.40 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 25779.68, NNZs: 4, Bias: 3272.368160, T: 72410140, Avg. loss: 15112761.714155\n",
      "Total training time: 9.45 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 25686.25, NNZs: 4, Bias: 3267.341191, T: 72836082, Avg. loss: 15048711.309103\n",
      "Total training time: 9.50 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 25581.71, NNZs: 4, Bias: 3262.264268, T: 73262024, Avg. loss: 15004149.643130\n",
      "Total training time: 9.55 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 25500.61, NNZs: 4, Bias: 3257.534952, T: 73687966, Avg. loss: 15109595.144402\n",
      "Total training time: 9.59 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 25406.59, NNZs: 4, Bias: 3252.586800, T: 74113908, Avg. loss: 14875391.716243\n",
      "Total training time: 9.64 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 25300.08, NNZs: 4, Bias: 3247.498786, T: 74539850, Avg. loss: 14772849.833543\n",
      "Total training time: 9.69 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 25209.03, NNZs: 4, Bias: 3242.702756, T: 74965792, Avg. loss: 14769043.546068\n",
      "Total training time: 9.74 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 25121.37, NNZs: 4, Bias: 3238.108389, T: 75391734, Avg. loss: 14964642.279838\n",
      "Total training time: 9.81 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 25034.57, NNZs: 4, Bias: 3233.305833, T: 75817676, Avg. loss: 14606654.350605\n",
      "Total training time: 9.88 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 24958.40, NNZs: 4, Bias: 3228.628850, T: 76243618, Avg. loss: 14415461.229608\n",
      "Total training time: 9.94 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 24867.95, NNZs: 4, Bias: 3223.811576, T: 76669560, Avg. loss: 14507839.455326\n",
      "Total training time: 10.01 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 24767.94, NNZs: 4, Bias: 3219.021011, T: 77095502, Avg. loss: 14334041.161651\n",
      "Total training time: 10.07 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 24682.66, NNZs: 4, Bias: 3214.379079, T: 77521444, Avg. loss: 14340025.967971\n",
      "Total training time: 10.13 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 24587.80, NNZs: 4, Bias: 3209.613157, T: 77947386, Avg. loss: 14179392.004367\n",
      "Total training time: 10.19 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 24504.66, NNZs: 4, Bias: 3204.849192, T: 78373328, Avg. loss: 14405636.456544\n",
      "Total training time: 10.24 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 24435.35, NNZs: 4, Bias: 3200.428093, T: 78799270, Avg. loss: 13877506.711495\n",
      "Total training time: 10.29 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 24339.12, NNZs: 4, Bias: 3195.640383, T: 79225212, Avg. loss: 13930557.762590\n",
      "Total training time: 10.35 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 24241.57, NNZs: 4, Bias: 3190.960512, T: 79651154, Avg. loss: 13867821.915162\n",
      "Total training time: 10.41 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 24156.10, NNZs: 4, Bias: 3186.280682, T: 80077096, Avg. loss: 13719660.318305\n",
      "Total training time: 10.47 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 24078.71, NNZs: 4, Bias: 3181.795192, T: 80503038, Avg. loss: 13904388.062525\n",
      "Total training time: 10.52 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 23991.69, NNZs: 4, Bias: 3177.217970, T: 80928980, Avg. loss: 13502344.005374\n",
      "Total training time: 10.57 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 23928.56, NNZs: 4, Bias: 3172.930623, T: 81354922, Avg. loss: 13683561.567440\n",
      "Total training time: 10.61 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 23857.12, NNZs: 4, Bias: 3168.635462, T: 81780864, Avg. loss: 13705872.101307\n",
      "Total training time: 10.66 seconds.\n",
      "-- Epoch 193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 23771.28, NNZs: 4, Bias: 3164.052889, T: 82206806, Avg. loss: 13400290.430784\n",
      "Total training time: 10.71 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 23702.90, NNZs: 4, Bias: 3159.952526, T: 82632748, Avg. loss: 13749395.320478\n",
      "Total training time: 10.76 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 23630.46, NNZs: 4, Bias: 3155.547468, T: 83058690, Avg. loss: 13181159.790499\n",
      "Total training time: 10.82 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 23555.62, NNZs: 4, Bias: 3151.224238, T: 83484632, Avg. loss: 13120238.189961\n",
      "Total training time: 10.88 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 23484.30, NNZs: 4, Bias: 3147.154727, T: 83910574, Avg. loss: 13270561.777593\n",
      "Total training time: 10.93 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 23415.88, NNZs: 4, Bias: 3142.936910, T: 84336516, Avg. loss: 13286576.978380\n",
      "Total training time: 10.98 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 23359.71, NNZs: 4, Bias: 3138.813156, T: 84762458, Avg. loss: 13115386.553938\n",
      "Total training time: 11.03 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 23276.74, NNZs: 4, Bias: 3134.391746, T: 85188400, Avg. loss: 12836041.029135\n",
      "Total training time: 11.09 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 23201.22, NNZs: 4, Bias: 3130.141408, T: 85614342, Avg. loss: 12858170.505079\n",
      "Total training time: 11.16 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 23147.90, NNZs: 4, Bias: 3126.091301, T: 86040284, Avg. loss: 12959745.681891\n",
      "Total training time: 11.22 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 23083.32, NNZs: 4, Bias: 3122.013122, T: 86466226, Avg. loss: 12866483.800506\n",
      "Total training time: 11.29 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 23003.08, NNZs: 4, Bias: 3117.741323, T: 86892168, Avg. loss: 12565819.187322\n",
      "Total training time: 11.35 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 22940.55, NNZs: 4, Bias: 3113.626263, T: 87318110, Avg. loss: 13007201.388169\n",
      "Total training time: 11.41 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 22881.75, NNZs: 4, Bias: 3109.713021, T: 87744052, Avg. loss: 13058876.445234\n",
      "Total training time: 11.46 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 22813.97, NNZs: 4, Bias: 3105.608207, T: 88169994, Avg. loss: 12626792.261194\n",
      "Total training time: 11.51 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 22756.33, NNZs: 4, Bias: 3101.647619, T: 88595936, Avg. loss: 12576406.330507\n",
      "Total training time: 11.55 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 22696.06, NNZs: 4, Bias: 3097.590929, T: 89021878, Avg. loss: 12243462.615127\n",
      "Total training time: 11.60 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 22633.94, NNZs: 4, Bias: 3093.705269, T: 89447820, Avg. loss: 12658930.168588\n",
      "Total training time: 11.65 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 22565.84, NNZs: 4, Bias: 3089.769785, T: 89873762, Avg. loss: 12298175.101827\n",
      "Total training time: 11.70 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 22494.14, NNZs: 4, Bias: 3085.580135, T: 90299704, Avg. loss: 12283754.157424\n",
      "Total training time: 11.74 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 22425.05, NNZs: 4, Bias: 3081.606936, T: 90725646, Avg. loss: 12176661.044208\n",
      "Total training time: 11.81 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 22367.91, NNZs: 4, Bias: 3077.763813, T: 91151588, Avg. loss: 12189603.640135\n",
      "Total training time: 11.89 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 22314.91, NNZs: 4, Bias: 3073.986475, T: 91577530, Avg. loss: 12323766.399602\n",
      "Total training time: 11.95 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 22257.77, NNZs: 4, Bias: 3070.102862, T: 92003472, Avg. loss: 12042916.216025\n",
      "Total training time: 12.01 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 22194.77, NNZs: 4, Bias: 3066.177187, T: 92429414, Avg. loss: 11930027.753589\n",
      "Total training time: 12.07 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 22135.70, NNZs: 4, Bias: 3062.333702, T: 92855356, Avg. loss: 11895014.763021\n",
      "Total training time: 12.12 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 22071.76, NNZs: 4, Bias: 3058.473771, T: 93281298, Avg. loss: 11878544.690513\n",
      "Total training time: 12.17 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 22016.36, NNZs: 4, Bias: 3054.832927, T: 93707240, Avg. loss: 11934740.127360\n",
      "Total training time: 12.21 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 21955.33, NNZs: 4, Bias: 3051.007478, T: 94133182, Avg. loss: 11728948.941670\n",
      "Total training time: 12.26 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 21888.82, NNZs: 4, Bias: 3047.196414, T: 94559124, Avg. loss: 11730811.963412\n",
      "Total training time: 12.31 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 21835.89, NNZs: 4, Bias: 3043.555383, T: 94985066, Avg. loss: 11709126.548972\n",
      "Total training time: 12.36 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 21784.51, NNZs: 4, Bias: 3040.001846, T: 95411008, Avg. loss: 11703395.873789\n",
      "Total training time: 12.41 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 21725.90, NNZs: 4, Bias: 3036.300702, T: 95836950, Avg. loss: 11615323.165801\n",
      "Total training time: 12.46 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 21666.94, NNZs: 4, Bias: 3032.590663, T: 96262892, Avg. loss: 11469950.973598\n",
      "Total training time: 12.52 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 21622.08, NNZs: 4, Bias: 3029.222964, T: 96688834, Avg. loss: 11712298.472912\n",
      "Total training time: 12.59 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 21570.41, NNZs: 4, Bias: 3025.622392, T: 97114776, Avg. loss: 11384495.677458\n",
      "Total training time: 12.64 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 21507.86, NNZs: 4, Bias: 3021.803687, T: 97540718, Avg. loss: 11289487.567129\n",
      "Total training time: 12.69 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 21442.71, NNZs: 4, Bias: 3017.952248, T: 97966660, Avg. loss: 11309430.602432\n",
      "Total training time: 12.76 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 21404.19, NNZs: 4, Bias: 3014.597086, T: 98392602, Avg. loss: 11414966.381490\n",
      "Total training time: 12.82 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 21358.56, NNZs: 4, Bias: 3011.031621, T: 98818544, Avg. loss: 11140040.709946\n",
      "Total training time: 12.88 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 21301.54, NNZs: 4, Bias: 3007.312896, T: 99244486, Avg. loss: 11167567.923833\n",
      "Total training time: 12.92 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 21253.40, NNZs: 4, Bias: 3003.874788, T: 99670428, Avg. loss: 11368913.734299\n",
      "Total training time: 12.97 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 21214.38, NNZs: 4, Bias: 3000.613631, T: 100096370, Avg. loss: 11332290.878406\n",
      "Total training time: 13.02 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 21162.16, NNZs: 4, Bias: 2997.095735, T: 100522312, Avg. loss: 10876146.042501\n",
      "Total training time: 13.07 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 21109.81, NNZs: 4, Bias: 2993.551662, T: 100948254, Avg. loss: 11018173.094552\n",
      "Total training time: 13.11 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 21058.39, NNZs: 4, Bias: 2990.172525, T: 101374196, Avg. loss: 10715476.668182\n",
      "Total training time: 13.15 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 21003.22, NNZs: 4, Bias: 2986.711502, T: 101800138, Avg. loss: 10995091.710383\n",
      "Total training time: 13.20 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 20961.62, NNZs: 4, Bias: 2983.404229, T: 102226080, Avg. loss: 10929428.753906\n",
      "Total training time: 13.25 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 20914.65, NNZs: 4, Bias: 2980.061272, T: 102652022, Avg. loss: 10731914.758825\n",
      "Total training time: 13.30 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 20883.88, NNZs: 4, Bias: 2976.757535, T: 103077964, Avg. loss: 10719663.591668\n",
      "Total training time: 13.34 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 20834.83, NNZs: 4, Bias: 2973.361732, T: 103503906, Avg. loss: 10712763.988012\n",
      "Total training time: 13.39 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 20784.79, NNZs: 4, Bias: 2969.989115, T: 103929848, Avg. loss: 10518527.256770\n",
      "Total training time: 13.43 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 20748.99, NNZs: 4, Bias: 2966.836963, T: 104355790, Avg. loss: 10808905.728451\n",
      "Total training time: 13.48 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 20704.52, NNZs: 4, Bias: 2963.590771, T: 104781732, Avg. loss: 10523269.629263\n",
      "Total training time: 13.53 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 20657.06, NNZs: 4, Bias: 2960.420408, T: 105207674, Avg. loss: 10609220.952985\n",
      "Total training time: 13.58 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 20608.81, NNZs: 4, Bias: 2957.096624, T: 105633616, Avg. loss: 10525302.581021\n",
      "Total training time: 13.62 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 20571.66, NNZs: 4, Bias: 2953.875852, T: 106059558, Avg. loss: 10466536.158248\n",
      "Total training time: 13.67 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 20514.45, NNZs: 4, Bias: 2950.488661, T: 106485500, Avg. loss: 10344806.345629\n",
      "Total training time: 13.71 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 20476.68, NNZs: 4, Bias: 2947.333853, T: 106911442, Avg. loss: 10487182.660325\n",
      "Total training time: 13.76 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 20427.31, NNZs: 4, Bias: 2944.022327, T: 107337384, Avg. loss: 10364073.417284\n",
      "Total training time: 13.81 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 20387.68, NNZs: 4, Bias: 2940.873763, T: 107763326, Avg. loss: 10304601.288787\n",
      "Total training time: 13.86 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 20340.08, NNZs: 4, Bias: 2937.684596, T: 108189268, Avg. loss: 10226063.294178\n",
      "Total training time: 13.91 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 20296.08, NNZs: 4, Bias: 2934.555244, T: 108615210, Avg. loss: 10193826.687953\n",
      "Total training time: 13.95 seconds.\n",
      "-- Epoch 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 20257.03, NNZs: 4, Bias: 2931.535441, T: 109041152, Avg. loss: 10365466.735425\n",
      "Total training time: 14.01 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 20210.79, NNZs: 4, Bias: 2928.251209, T: 109467094, Avg. loss: 10041529.815478\n",
      "Total training time: 14.05 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 20177.07, NNZs: 4, Bias: 2925.198858, T: 109893036, Avg. loss: 10060464.894026\n",
      "Total training time: 14.10 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 20143.44, NNZs: 4, Bias: 2922.211239, T: 110318978, Avg. loss: 10266840.107304\n",
      "Total training time: 14.14 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 20101.85, NNZs: 4, Bias: 2919.132336, T: 110744920, Avg. loss: 9891188.830430\n",
      "Total training time: 14.19 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 20062.49, NNZs: 4, Bias: 2916.151944, T: 111170862, Avg. loss: 10015888.981862\n",
      "Total training time: 14.24 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 20021.89, NNZs: 4, Bias: 2913.171231, T: 111596804, Avg. loss: 9943160.505849\n",
      "Total training time: 14.28 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 19978.46, NNZs: 4, Bias: 2910.165231, T: 112022746, Avg. loss: 9879398.471419\n",
      "Total training time: 14.33 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 19937.20, NNZs: 4, Bias: 2907.129358, T: 112448688, Avg. loss: 9833152.322026\n",
      "Total training time: 14.38 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 19895.23, NNZs: 4, Bias: 2904.061915, T: 112874630, Avg. loss: 9855909.385643\n",
      "Total training time: 14.43 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 19862.47, NNZs: 4, Bias: 2901.091569, T: 113300572, Avg. loss: 9902035.276638\n",
      "Total training time: 14.48 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 19832.54, NNZs: 4, Bias: 2898.203924, T: 113726514, Avg. loss: 10106191.260799\n",
      "Total training time: 14.52 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 19793.00, NNZs: 4, Bias: 2895.229176, T: 114152456, Avg. loss: 9719175.762140\n",
      "Total training time: 14.57 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 19759.22, NNZs: 4, Bias: 2892.337266, T: 114578398, Avg. loss: 9542768.832752\n",
      "Total training time: 14.61 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 19718.82, NNZs: 4, Bias: 2889.320019, T: 115004340, Avg. loss: 9624124.317878\n",
      "Total training time: 14.66 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 19681.11, NNZs: 4, Bias: 2886.326264, T: 115430282, Avg. loss: 9580222.996930\n",
      "Total training time: 14.71 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 19650.13, NNZs: 4, Bias: 2883.634078, T: 115856224, Avg. loss: 9694254.568977\n",
      "Total training time: 14.76 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 19624.90, NNZs: 4, Bias: 2880.902990, T: 116282166, Avg. loss: 9534025.467183\n",
      "Total training time: 14.80 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 19584.67, NNZs: 4, Bias: 2878.041507, T: 116708108, Avg. loss: 9603227.363516\n",
      "Total training time: 14.85 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 19550.71, NNZs: 4, Bias: 2875.235895, T: 117134050, Avg. loss: 9550006.003547\n",
      "Total training time: 14.90 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 19509.92, NNZs: 4, Bias: 2872.314088, T: 117559992, Avg. loss: 9395979.042722\n",
      "Total training time: 14.95 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 19473.15, NNZs: 4, Bias: 2869.473258, T: 117985934, Avg. loss: 9304191.674221\n",
      "Total training time: 15.00 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 19434.08, NNZs: 4, Bias: 2866.483192, T: 118411876, Avg. loss: 9334318.131227\n",
      "Total training time: 15.04 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 19405.76, NNZs: 4, Bias: 2863.820961, T: 118837818, Avg. loss: 9568467.220302\n",
      "Total training time: 15.09 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 19367.31, NNZs: 4, Bias: 2860.905786, T: 119263760, Avg. loss: 9351888.276835\n",
      "Total training time: 15.14 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 19335.04, NNZs: 4, Bias: 2858.270856, T: 119689702, Avg. loss: 9230913.467687\n",
      "Total training time: 15.19 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 19300.43, NNZs: 4, Bias: 2855.482939, T: 120115644, Avg. loss: 9190020.423020\n",
      "Total training time: 15.24 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 19268.95, NNZs: 4, Bias: 2852.660399, T: 120541586, Avg. loss: 9262614.339885\n",
      "Total training time: 15.28 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 19237.40, NNZs: 4, Bias: 2849.996959, T: 120967528, Avg. loss: 9168435.265686\n",
      "Total training time: 15.33 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 19206.07, NNZs: 4, Bias: 2847.384569, T: 121393470, Avg. loss: 9046020.881696\n",
      "Total training time: 15.37 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 19175.52, NNZs: 4, Bias: 2844.761861, T: 121819412, Avg. loss: 9098181.542932\n",
      "Total training time: 15.43 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 19140.95, NNZs: 4, Bias: 2842.013035, T: 122245354, Avg. loss: 9043942.153624\n",
      "Total training time: 15.47 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 19098.29, NNZs: 4, Bias: 2839.174337, T: 122671296, Avg. loss: 8965078.056779\n",
      "Total training time: 15.52 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 19070.81, NNZs: 4, Bias: 2836.507410, T: 123097238, Avg. loss: 9229814.511073\n",
      "Total training time: 15.56 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 19043.35, NNZs: 4, Bias: 2833.957303, T: 123523180, Avg. loss: 8947409.209941\n",
      "Total training time: 15.61 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 19005.95, NNZs: 4, Bias: 2831.225782, T: 123949122, Avg. loss: 8916578.937836\n",
      "Total training time: 15.66 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 18979.57, NNZs: 4, Bias: 2828.643089, T: 124375064, Avg. loss: 8886272.036317\n",
      "Total training time: 15.72 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 18955.97, NNZs: 4, Bias: 2825.944463, T: 124801006, Avg. loss: 8854271.908638\n",
      "Total training time: 15.79 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 18920.89, NNZs: 4, Bias: 2823.234007, T: 125226948, Avg. loss: 8885376.488687\n",
      "Total training time: 15.86 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 18889.96, NNZs: 4, Bias: 2820.712749, T: 125652890, Avg. loss: 8740638.596764\n",
      "Total training time: 15.92 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 18856.51, NNZs: 4, Bias: 2818.034693, T: 126078832, Avg. loss: 8875236.394954\n",
      "Total training time: 15.96 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 18812.84, NNZs: 4, Bias: 2815.280496, T: 126504774, Avg. loss: 8771511.127129\n",
      "Total training time: 16.01 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 18789.52, NNZs: 4, Bias: 2812.714669, T: 126930716, Avg. loss: 8737262.035894\n",
      "Total training time: 16.06 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 18754.87, NNZs: 4, Bias: 2810.037124, T: 127356658, Avg. loss: 8703644.167673\n",
      "Total training time: 16.11 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 18722.04, NNZs: 4, Bias: 2807.442504, T: 127782600, Avg. loss: 8672992.217715\n",
      "Total training time: 16.16 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parv/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='modified_huber', max_iter=300, n_jobs=-1, verbose=2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDClassifier(loss='modified_huber', verbose=2, n_jobs=-1, max_iter=300)\n",
    "sgd.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7397592171740887"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parv/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 58600.61, NNZs: 4, Bias: 1371.899732, T: 425942, Avg. loss: 1161945953.539370\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 56432.61, NNZs: 4, Bias: 1476.146081, T: 851884, Avg. loss: 118382091.342429\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 53933.96, NNZs: 4, Bias: 1520.983406, T: 1277826, Avg. loss: 67795057.474828\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 52094.07, NNZs: 4, Bias: 1545.735091, T: 1703768, Avg. loss: 48653980.021929\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 50028.35, NNZs: 4, Bias: 1555.989410, T: 2129710, Avg. loss: 36484807.421644\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 48073.71, NNZs: 4, Bias: 1559.227552, T: 2555652, Avg. loss: 29889072.162555\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 46440.23, NNZs: 4, Bias: 1561.175263, T: 2981594, Avg. loss: 25428088.075450\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 44936.93, NNZs: 4, Bias: 1562.003690, T: 3407536, Avg. loss: 22222252.076583\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 43505.42, NNZs: 4, Bias: 1558.940572, T: 3833478, Avg. loss: 19488380.019248\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 41985.72, NNZs: 4, Bias: 1553.034249, T: 4259420, Avg. loss: 17293000.663700\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 40698.23, NNZs: 4, Bias: 1548.270224, T: 4685362, Avg. loss: 15779208.654037\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 39393.63, NNZs: 4, Bias: 1541.143154, T: 5111304, Avg. loss: 14176157.833206\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 38213.98, NNZs: 4, Bias: 1534.695637, T: 5537246, Avg. loss: 13275759.623418\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 37084.74, NNZs: 4, Bias: 1527.638660, T: 5963188, Avg. loss: 12284003.863967\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 35994.59, NNZs: 4, Bias: 1520.242085, T: 6389130, Avg. loss: 11311589.605667\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 34923.29, NNZs: 4, Bias: 1512.408057, T: 6815072, Avg. loss: 10401380.526008\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 33914.61, NNZs: 4, Bias: 1504.835649, T: 7241014, Avg. loss: 9928382.271105\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 32969.46, NNZs: 4, Bias: 1497.262790, T: 7666956, Avg. loss: 9333230.345332\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 32121.20, NNZs: 4, Bias: 1490.002945, T: 8092898, Avg. loss: 8844757.336736\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 31263.30, NNZs: 4, Bias: 1482.200790, T: 8518840, Avg. loss: 8396251.263427\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 30414.70, NNZs: 4, Bias: 1474.342460, T: 8944782, Avg. loss: 7930995.140441\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 29694.01, NNZs: 4, Bias: 1467.537636, T: 9370724, Avg. loss: 7640310.671096\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 28970.80, NNZs: 4, Bias: 1460.514737, T: 9796666, Avg. loss: 7263627.254347\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 28241.00, NNZs: 4, Bias: 1452.947084, T: 10222608, Avg. loss: 6834935.948245\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 27559.68, NNZs: 4, Bias: 1445.696032, T: 10648550, Avg. loss: 6601501.218638\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 26927.88, NNZs: 4, Bias: 1438.917164, T: 11074492, Avg. loss: 6455827.908618\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 26283.41, NNZs: 4, Bias: 1431.819039, T: 11500434, Avg. loss: 6223364.745258\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 25673.31, NNZs: 4, Bias: 1424.835850, T: 11926376, Avg. loss: 5935208.056539\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 25084.60, NNZs: 4, Bias: 1417.969697, T: 12352318, Avg. loss: 5720188.085839\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 24546.81, NNZs: 4, Bias: 1411.472460, T: 12778260, Avg. loss: 5501939.208925\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 24032.58, NNZs: 4, Bias: 1405.035975, T: 13204202, Avg. loss: 5259710.460057\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 23539.49, NNZs: 4, Bias: 1398.957546, T: 13630144, Avg. loss: 5308211.442581\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 23064.13, NNZs: 4, Bias: 1392.809617, T: 14056086, Avg. loss: 5132910.265803\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 22581.20, NNZs: 4, Bias: 1386.522969, T: 14482028, Avg. loss: 4804236.851477\n",
      "Total training time: 1.72 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 22151.30, NNZs: 4, Bias: 1380.766545, T: 14907970, Avg. loss: 4704648.130610\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 21694.97, NNZs: 4, Bias: 1374.674305, T: 15333912, Avg. loss: 4558274.512787\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 21267.57, NNZs: 4, Bias: 1368.885462, T: 15759854, Avg. loss: 4460027.066553\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 20868.75, NNZs: 4, Bias: 1363.219048, T: 16185796, Avg. loss: 4373966.077474\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 20475.89, NNZs: 4, Bias: 1357.678873, T: 16611738, Avg. loss: 4257912.896626\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 20072.88, NNZs: 4, Bias: 1351.986454, T: 17037680, Avg. loss: 4092007.229621\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 19728.83, NNZs: 4, Bias: 1346.885625, T: 17463622, Avg. loss: 4009157.203024\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 19366.23, NNZs: 4, Bias: 1341.664216, T: 17889564, Avg. loss: 3910391.901064\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 19013.41, NNZs: 4, Bias: 1336.518427, T: 18315506, Avg. loss: 3873218.542886\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 18676.47, NNZs: 4, Bias: 1331.415287, T: 18741448, Avg. loss: 3714657.879161\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 18374.68, NNZs: 4, Bias: 1326.481860, T: 19167390, Avg. loss: 3631235.025381\n",
      "Total training time: 2.28 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 18059.40, NNZs: 4, Bias: 1321.440587, T: 19593332, Avg. loss: 3497590.136359\n",
      "Total training time: 2.33 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 17768.45, NNZs: 4, Bias: 1316.511065, T: 20019274, Avg. loss: 3509596.563732\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 17481.05, NNZs: 4, Bias: 1311.774122, T: 20445216, Avg. loss: 3423641.527308\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 17207.64, NNZs: 4, Bias: 1307.078881, T: 20871158, Avg. loss: 3320025.664057\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 16955.25, NNZs: 4, Bias: 1302.686163, T: 21297100, Avg. loss: 3369882.561520\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 16682.37, NNZs: 4, Bias: 1298.167045, T: 21723042, Avg. loss: 3209336.768714\n",
      "Total training time: 2.58 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 16420.76, NNZs: 4, Bias: 1293.624950, T: 22148984, Avg. loss: 3089027.027710\n",
      "Total training time: 2.63 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 16161.19, NNZs: 4, Bias: 1289.138712, T: 22574926, Avg. loss: 3101309.936890\n",
      "Total training time: 2.68 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 15916.46, NNZs: 4, Bias: 1284.712335, T: 23000868, Avg. loss: 3038362.040586\n",
      "Total training time: 2.73 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 15682.26, NNZs: 4, Bias: 1280.405291, T: 23426810, Avg. loss: 3055165.392885\n",
      "Total training time: 2.79 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 15439.05, NNZs: 4, Bias: 1276.106319, T: 23852752, Avg. loss: 2919216.713022\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 15217.30, NNZs: 4, Bias: 1272.117093, T: 24278694, Avg. loss: 2881635.941507\n",
      "Total training time: 2.88 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 15013.94, NNZs: 4, Bias: 1268.135205, T: 24704636, Avg. loss: 2831847.729791\n",
      "Total training time: 2.93 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 14803.14, NNZs: 4, Bias: 1264.056862, T: 25130578, Avg. loss: 2778095.718101\n",
      "Total training time: 2.98 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 14598.24, NNZs: 4, Bias: 1260.087524, T: 25556520, Avg. loss: 2715429.192846\n",
      "Total training time: 3.05 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 14396.49, NNZs: 4, Bias: 1256.098897, T: 25982462, Avg. loss: 2669208.937413\n",
      "Total training time: 3.12 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 14194.37, NNZs: 4, Bias: 1252.260164, T: 26408404, Avg. loss: 2632771.961035\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 14013.14, NNZs: 4, Bias: 1248.497942, T: 26834346, Avg. loss: 2613304.393233\n",
      "Total training time: 3.21 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 13832.76, NNZs: 4, Bias: 1244.884692, T: 27260288, Avg. loss: 2551339.495105\n",
      "Total training time: 3.26 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 13653.70, NNZs: 4, Bias: 1241.270709, T: 27686230, Avg. loss: 2529416.811589\n",
      "Total training time: 3.31 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 13483.72, NNZs: 4, Bias: 1237.726735, T: 28112172, Avg. loss: 2448003.058041\n",
      "Total training time: 3.36 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 13313.04, NNZs: 4, Bias: 1234.197127, T: 28538114, Avg. loss: 2428258.675365\n",
      "Total training time: 3.41 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 13155.71, NNZs: 4, Bias: 1230.778724, T: 28964056, Avg. loss: 2433063.840791\n",
      "Total training time: 3.45 seconds.\n",
      "-- Epoch 69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 12998.19, NNZs: 4, Bias: 1227.343006, T: 29389998, Avg. loss: 2374578.899145\n",
      "Total training time: 3.50 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 12849.88, NNZs: 4, Bias: 1224.059103, T: 29815940, Avg. loss: 2395621.140318\n",
      "Total training time: 3.55 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 12697.75, NNZs: 4, Bias: 1220.806584, T: 30241882, Avg. loss: 2292199.643003\n",
      "Total training time: 3.59 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 12543.60, NNZs: 4, Bias: 1217.486330, T: 30667824, Avg. loss: 2250651.287390\n",
      "Total training time: 3.64 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 12398.98, NNZs: 4, Bias: 1214.244773, T: 31093766, Avg. loss: 2233595.357756\n",
      "Total training time: 3.69 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 12254.06, NNZs: 4, Bias: 1211.024015, T: 31519708, Avg. loss: 2233683.620323\n",
      "Total training time: 3.74 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 12108.99, NNZs: 4, Bias: 1207.841756, T: 31945650, Avg. loss: 2175940.080131\n",
      "Total training time: 3.79 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 11982.86, NNZs: 4, Bias: 1204.770780, T: 32371592, Avg. loss: 2164101.818468\n",
      "Total training time: 3.84 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 11854.84, NNZs: 4, Bias: 1201.733253, T: 32797534, Avg. loss: 2112273.853280\n",
      "Total training time: 3.91 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 11722.96, NNZs: 4, Bias: 1198.633768, T: 33223476, Avg. loss: 2100775.613281\n",
      "Total training time: 3.98 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 11598.45, NNZs: 4, Bias: 1195.667180, T: 33649418, Avg. loss: 2118902.963399\n",
      "Total training time: 4.03 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 11471.78, NNZs: 4, Bias: 1192.641761, T: 34075360, Avg. loss: 2027425.513743\n",
      "Total training time: 4.08 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 11342.17, NNZs: 4, Bias: 1189.635618, T: 34501302, Avg. loss: 2029085.661337\n",
      "Total training time: 4.12 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 11227.15, NNZs: 4, Bias: 1186.737148, T: 34927244, Avg. loss: 1984363.318034\n",
      "Total training time: 4.17 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 11114.87, NNZs: 4, Bias: 1183.976383, T: 35353186, Avg. loss: 1956969.171150\n",
      "Total training time: 4.22 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 11003.67, NNZs: 4, Bias: 1181.156215, T: 35779128, Avg. loss: 1937555.831391\n",
      "Total training time: 4.27 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 10890.04, NNZs: 4, Bias: 1178.380990, T: 36205070, Avg. loss: 1944124.228300\n",
      "Total training time: 4.32 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 10782.33, NNZs: 4, Bias: 1175.637813, T: 36631012, Avg. loss: 1880909.494772\n",
      "Total training time: 4.36 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 10673.39, NNZs: 4, Bias: 1172.862757, T: 37056954, Avg. loss: 1876945.595599\n",
      "Total training time: 4.41 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 10572.93, NNZs: 4, Bias: 1170.248813, T: 37482896, Avg. loss: 1835146.189372\n",
      "Total training time: 4.46 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 10475.61, NNZs: 4, Bias: 1167.559641, T: 37908838, Avg. loss: 1838002.110582\n",
      "Total training time: 4.51 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 10374.32, NNZs: 4, Bias: 1164.874073, T: 38334780, Avg. loss: 1819918.997021\n",
      "Total training time: 4.56 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 10282.60, NNZs: 4, Bias: 1162.287873, T: 38760722, Avg. loss: 1766255.908665\n",
      "Total training time: 4.61 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 10196.92, NNZs: 4, Bias: 1159.768613, T: 39186664, Avg. loss: 1781157.509597\n",
      "Total training time: 4.66 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 10110.95, NNZs: 4, Bias: 1157.283777, T: 39612606, Avg. loss: 1751886.074432\n",
      "Total training time: 4.71 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 10025.96, NNZs: 4, Bias: 1154.864292, T: 40038548, Avg. loss: 1738743.657299\n",
      "Total training time: 4.76 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 9940.73, NNZs: 4, Bias: 1152.437221, T: 40464490, Avg. loss: 1719112.019590\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 9856.54, NNZs: 4, Bias: 1150.008568, T: 40890432, Avg. loss: 1691217.649637\n",
      "Total training time: 4.85 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 9781.71, NNZs: 4, Bias: 1147.792824, T: 41316374, Avg. loss: 1744417.410435\n",
      "Total training time: 4.91 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 9702.34, NNZs: 4, Bias: 1145.402995, T: 41742316, Avg. loss: 1690329.243713\n",
      "Total training time: 4.97 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 9625.86, NNZs: 4, Bias: 1143.069001, T: 42168258, Avg. loss: 1643942.794877\n",
      "Total training time: 5.02 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 9550.92, NNZs: 4, Bias: 1140.758347, T: 42594200, Avg. loss: 1629368.598919\n",
      "Total training time: 5.06 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 9474.75, NNZs: 4, Bias: 1138.435581, T: 43020142, Avg. loss: 1581623.823308\n",
      "Total training time: 5.12 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 9402.17, NNZs: 4, Bias: 1136.168026, T: 43446084, Avg. loss: 1593998.842780\n",
      "Total training time: 5.18 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 9324.20, NNZs: 4, Bias: 1133.807947, T: 43872026, Avg. loss: 1591953.642350\n",
      "Total training time: 5.23 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 9255.07, NNZs: 4, Bias: 1131.633580, T: 44297968, Avg. loss: 1569420.062432\n",
      "Total training time: 5.28 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 9186.40, NNZs: 4, Bias: 1129.429142, T: 44723910, Avg. loss: 1580545.239197\n",
      "Total training time: 5.34 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 9122.63, NNZs: 4, Bias: 1127.231797, T: 45149852, Avg. loss: 1554205.616931\n",
      "Total training time: 5.39 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 9054.83, NNZs: 4, Bias: 1125.051833, T: 45575794, Avg. loss: 1535186.614321\n",
      "Total training time: 5.44 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 8980.67, NNZs: 4, Bias: 1122.795611, T: 46001736, Avg. loss: 1484675.112315\n",
      "Total training time: 5.49 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 8910.46, NNZs: 4, Bias: 1120.589847, T: 46427678, Avg. loss: 1480943.025940\n",
      "Total training time: 5.54 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 8850.48, NNZs: 4, Bias: 1118.578703, T: 46853620, Avg. loss: 1514784.766379\n",
      "Total training time: 5.60 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 8778.73, NNZs: 4, Bias: 1116.379038, T: 47279562, Avg. loss: 1460039.043456\n",
      "Total training time: 5.67 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 8717.72, NNZs: 4, Bias: 1114.306577, T: 47705504, Avg. loss: 1455633.784561\n",
      "Total training time: 5.74 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 8657.26, NNZs: 4, Bias: 1112.267725, T: 48131446, Avg. loss: 1446260.123022\n",
      "Total training time: 5.82 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 8598.13, NNZs: 4, Bias: 1110.257576, T: 48557388, Avg. loss: 1423370.756073\n",
      "Total training time: 5.89 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 8538.68, NNZs: 4, Bias: 1108.251490, T: 48983330, Avg. loss: 1422381.183122\n",
      "Total training time: 5.95 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 8476.03, NNZs: 4, Bias: 1106.222440, T: 49409272, Avg. loss: 1396260.721419\n",
      "Total training time: 6.01 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 8429.98, NNZs: 4, Bias: 1104.331771, T: 49835214, Avg. loss: 1386742.399400\n",
      "Total training time: 6.06 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 8383.34, NNZs: 4, Bias: 1102.497509, T: 50261156, Avg. loss: 1390580.920020\n",
      "Total training time: 6.11 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 8329.37, NNZs: 4, Bias: 1100.589520, T: 50687098, Avg. loss: 1370886.053879\n",
      "Total training time: 6.16 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 8273.24, NNZs: 4, Bias: 1098.621401, T: 51113040, Avg. loss: 1355248.827059\n",
      "Total training time: 6.20 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 8222.48, NNZs: 4, Bias: 1096.734294, T: 51538982, Avg. loss: 1342622.306967\n",
      "Total training time: 6.25 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 8161.73, NNZs: 4, Bias: 1094.778286, T: 51964924, Avg. loss: 1341075.768305\n",
      "Total training time: 6.30 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 8112.59, NNZs: 4, Bias: 1092.935900, T: 52390866, Avg. loss: 1323850.570308\n",
      "Total training time: 6.35 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 8066.48, NNZs: 4, Bias: 1091.135922, T: 52816808, Avg. loss: 1326461.425305\n",
      "Total training time: 6.41 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 8017.72, NNZs: 4, Bias: 1089.310670, T: 53242750, Avg. loss: 1321831.588610\n",
      "Total training time: 6.47 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 7981.64, NNZs: 4, Bias: 1087.592198, T: 53668692, Avg. loss: 1330055.458386\n",
      "Total training time: 6.52 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 7932.79, NNZs: 4, Bias: 1085.750302, T: 54094634, Avg. loss: 1276213.647493\n",
      "Total training time: 6.57 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 7886.07, NNZs: 4, Bias: 1083.972261, T: 54520576, Avg. loss: 1259582.202564\n",
      "Total training time: 6.62 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 7836.40, NNZs: 4, Bias: 1082.137301, T: 54946518, Avg. loss: 1258869.015052\n",
      "Total training time: 6.67 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 7793.40, NNZs: 4, Bias: 1080.421437, T: 55372460, Avg. loss: 1273666.548253\n",
      "Total training time: 6.72 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 7751.21, NNZs: 4, Bias: 1078.712170, T: 55798402, Avg. loss: 1250599.983151\n",
      "Total training time: 6.77 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 7705.68, NNZs: 4, Bias: 1076.976113, T: 56224344, Avg. loss: 1240047.501617\n",
      "Total training time: 6.82 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 7659.12, NNZs: 4, Bias: 1075.236933, T: 56650286, Avg. loss: 1235558.813738\n",
      "Total training time: 6.88 seconds.\n",
      "-- Epoch 134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 7617.94, NNZs: 4, Bias: 1073.550848, T: 57076228, Avg. loss: 1239803.564834\n",
      "Total training time: 6.95 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 7578.72, NNZs: 4, Bias: 1071.900049, T: 57502170, Avg. loss: 1206134.418847\n",
      "Total training time: 7.01 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 7535.92, NNZs: 4, Bias: 1070.208358, T: 57928112, Avg. loss: 1194915.829635\n",
      "Total training time: 7.07 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 7490.76, NNZs: 4, Bias: 1068.526462, T: 58354054, Avg. loss: 1181943.850010\n",
      "Total training time: 7.12 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 7448.78, NNZs: 4, Bias: 1066.895929, T: 58779996, Avg. loss: 1162897.256741\n",
      "Total training time: 7.17 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 7409.99, NNZs: 4, Bias: 1065.254420, T: 59205938, Avg. loss: 1177529.119956\n",
      "Total training time: 7.22 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 7375.13, NNZs: 4, Bias: 1063.659554, T: 59631880, Avg. loss: 1178108.305058\n",
      "Total training time: 7.27 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 7332.46, NNZs: 4, Bias: 1062.008200, T: 60057822, Avg. loss: 1153456.686601\n",
      "Total training time: 7.32 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 7303.31, NNZs: 4, Bias: 1060.553877, T: 60483764, Avg. loss: 1177868.239453\n",
      "Total training time: 7.36 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 7270.79, NNZs: 4, Bias: 1059.029707, T: 60909706, Avg. loss: 1145264.588382\n",
      "Total training time: 7.42 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 7233.86, NNZs: 4, Bias: 1057.451461, T: 61335648, Avg. loss: 1129427.092504\n",
      "Total training time: 7.47 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 7195.78, NNZs: 4, Bias: 1055.884263, T: 61761590, Avg. loss: 1128398.769730\n",
      "Total training time: 7.51 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 7153.78, NNZs: 4, Bias: 1054.289553, T: 62187532, Avg. loss: 1114260.378156\n",
      "Total training time: 7.56 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 7111.38, NNZs: 4, Bias: 1052.696372, T: 62613474, Avg. loss: 1106142.382244\n",
      "Total training time: 7.61 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 7074.01, NNZs: 4, Bias: 1051.124089, T: 63039416, Avg. loss: 1097707.837849\n",
      "Total training time: 7.66 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 7041.05, NNZs: 4, Bias: 1049.631270, T: 63465358, Avg. loss: 1097230.356358\n",
      "Total training time: 7.71 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 7015.29, NNZs: 4, Bias: 1048.232953, T: 63891300, Avg. loss: 1103102.100601\n",
      "Total training time: 7.77 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 6975.38, NNZs: 4, Bias: 1046.673360, T: 64317242, Avg. loss: 1085923.647716\n",
      "Total training time: 7.82 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 6942.22, NNZs: 4, Bias: 1045.192544, T: 64743184, Avg. loss: 1071396.760973\n",
      "Total training time: 7.89 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 6908.48, NNZs: 4, Bias: 1043.704709, T: 65169126, Avg. loss: 1059323.417767\n",
      "Total training time: 7.96 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 6876.84, NNZs: 4, Bias: 1042.264243, T: 65595068, Avg. loss: 1057221.655020\n",
      "Total training time: 8.01 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 6846.04, NNZs: 4, Bias: 1040.829323, T: 66021010, Avg. loss: 1063178.081392\n",
      "Total training time: 8.06 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 6813.82, NNZs: 4, Bias: 1039.408215, T: 66446952, Avg. loss: 1060948.609387\n",
      "Total training time: 8.11 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 6780.59, NNZs: 4, Bias: 1037.985480, T: 66872894, Avg. loss: 1039378.317340\n",
      "Total training time: 8.16 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 6750.92, NNZs: 4, Bias: 1036.609195, T: 67298836, Avg. loss: 1025240.411658\n",
      "Total training time: 8.21 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 6724.01, NNZs: 4, Bias: 1035.275581, T: 67724778, Avg. loss: 1023851.841356\n",
      "Total training time: 8.26 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 6702.06, NNZs: 4, Bias: 1034.005826, T: 68150720, Avg. loss: 1038247.144047\n",
      "Total training time: 8.31 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 6677.49, NNZs: 4, Bias: 1032.620976, T: 68576662, Avg. loss: 1009044.759074\n",
      "Total training time: 8.36 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 6646.53, NNZs: 4, Bias: 1031.282032, T: 69002604, Avg. loss: 1021656.903112\n",
      "Total training time: 8.41 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 6625.33, NNZs: 4, Bias: 1030.007808, T: 69428546, Avg. loss: 1019659.811066\n",
      "Total training time: 8.46 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 6596.89, NNZs: 4, Bias: 1028.650216, T: 69854488, Avg. loss: 984558.608435\n",
      "Total training time: 8.50 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 6569.27, NNZs: 4, Bias: 1027.333458, T: 70280430, Avg. loss: 988252.063323\n",
      "Total training time: 8.55 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 6539.22, NNZs: 4, Bias: 1025.985590, T: 70706372, Avg. loss: 968698.974181\n",
      "Total training time: 8.61 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 6518.90, NNZs: 4, Bias: 1024.760688, T: 71132314, Avg. loss: 960874.373143\n",
      "Total training time: 8.68 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 6498.94, NNZs: 4, Bias: 1023.529592, T: 71558256, Avg. loss: 994931.739587\n",
      "Total training time: 8.76 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 6477.73, NNZs: 4, Bias: 1022.317110, T: 71984198, Avg. loss: 979388.679495\n",
      "Total training time: 8.84 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 6450.80, NNZs: 4, Bias: 1021.042217, T: 72410140, Avg. loss: 965971.202682\n",
      "Total training time: 8.92 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 6420.83, NNZs: 4, Bias: 1019.719077, T: 72836082, Avg. loss: 959994.885907\n",
      "Total training time: 8.98 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 6398.47, NNZs: 4, Bias: 1018.495322, T: 73262024, Avg. loss: 954061.905662\n",
      "Total training time: 9.04 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 6371.65, NNZs: 4, Bias: 1017.223138, T: 73687966, Avg. loss: 929151.046767\n",
      "Total training time: 9.10 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 6347.70, NNZs: 4, Bias: 1015.957962, T: 74113908, Avg. loss: 947666.345302\n",
      "Total training time: 9.15 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 6321.00, NNZs: 4, Bias: 1014.659675, T: 74539850, Avg. loss: 935250.323821\n",
      "Total training time: 9.20 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 6299.38, NNZs: 4, Bias: 1013.441758, T: 74965792, Avg. loss: 920458.675217\n",
      "Total training time: 9.26 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 6270.99, NNZs: 4, Bias: 1012.142232, T: 75391734, Avg. loss: 916003.102770\n",
      "Total training time: 9.32 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 6244.48, NNZs: 4, Bias: 1010.878016, T: 75817676, Avg. loss: 913429.701119\n",
      "Total training time: 9.41 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 6224.10, NNZs: 4, Bias: 1009.705259, T: 76243618, Avg. loss: 895452.893737\n",
      "Total training time: 9.49 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 6197.08, NNZs: 4, Bias: 1008.428011, T: 76669560, Avg. loss: 892502.762628\n",
      "Total training time: 9.57 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 6174.12, NNZs: 4, Bias: 1007.238269, T: 77095502, Avg. loss: 887476.830318\n",
      "Total training time: 9.64 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 6151.41, NNZs: 4, Bias: 1006.024310, T: 77521444, Avg. loss: 902627.699840\n",
      "Total training time: 9.71 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 6131.94, NNZs: 4, Bias: 1004.850431, T: 77947386, Avg. loss: 899456.666433\n",
      "Total training time: 9.77 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 6112.22, NNZs: 4, Bias: 1003.696179, T: 78373328, Avg. loss: 899570.009981\n",
      "Total training time: 9.82 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 6097.27, NNZs: 4, Bias: 1002.616326, T: 78799270, Avg. loss: 891089.777407\n",
      "Total training time: 9.88 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 6074.78, NNZs: 4, Bias: 1001.446736, T: 79225212, Avg. loss: 879976.779455\n",
      "Total training time: 9.94 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 6057.70, NNZs: 4, Bias: 1000.298797, T: 79651154, Avg. loss: 881730.054757\n",
      "Total training time: 9.99 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 6041.01, NNZs: 4, Bias: 999.182212, T: 80077096, Avg. loss: 871673.800809\n",
      "Total training time: 10.05 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 6018.79, NNZs: 4, Bias: 998.031002, T: 80503038, Avg. loss: 856436.804566\n",
      "Total training time: 10.10 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 5998.83, NNZs: 4, Bias: 996.908520, T: 80928980, Avg. loss: 859996.934842\n",
      "Total training time: 10.15 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 5981.17, NNZs: 4, Bias: 995.799459, T: 81354922, Avg. loss: 835531.737279\n",
      "Total training time: 10.21 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 5961.09, NNZs: 4, Bias: 994.668874, T: 81780864, Avg. loss: 841735.480172\n",
      "Total training time: 10.25 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 5943.57, NNZs: 4, Bias: 993.578339, T: 82206806, Avg. loss: 859427.701136\n",
      "Total training time: 10.30 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 5923.26, NNZs: 4, Bias: 992.499618, T: 82632748, Avg. loss: 830048.445421\n",
      "Total training time: 10.37 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 5905.07, NNZs: 4, Bias: 991.398873, T: 83058690, Avg. loss: 839999.771672\n",
      "Total training time: 10.43 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 5886.60, NNZs: 4, Bias: 990.284868, T: 83484632, Avg. loss: 829791.244592\n",
      "Total training time: 10.47 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 5864.98, NNZs: 4, Bias: 989.166961, T: 83910574, Avg. loss: 817912.856726\n",
      "Total training time: 10.52 seconds.\n",
      "-- Epoch 198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 5846.70, NNZs: 4, Bias: 988.129067, T: 84336516, Avg. loss: 831648.655644\n",
      "Total training time: 10.58 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 5828.90, NNZs: 4, Bias: 987.057502, T: 84762458, Avg. loss: 823130.326797\n",
      "Total training time: 10.63 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 5811.81, NNZs: 4, Bias: 985.998101, T: 85188400, Avg. loss: 804433.202904\n",
      "Total training time: 10.68 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 5795.83, NNZs: 4, Bias: 984.980267, T: 85614342, Avg. loss: 802745.479459\n",
      "Total training time: 10.72 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 5781.29, NNZs: 4, Bias: 983.955055, T: 86040284, Avg. loss: 809685.427841\n",
      "Total training time: 10.77 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 5764.15, NNZs: 4, Bias: 982.905242, T: 86466226, Avg. loss: 797032.532607\n",
      "Total training time: 10.82 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 5746.78, NNZs: 4, Bias: 981.882134, T: 86892168, Avg. loss: 797096.128125\n",
      "Total training time: 10.87 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 5729.90, NNZs: 4, Bias: 980.816137, T: 87318110, Avg. loss: 797603.863969\n",
      "Total training time: 10.91 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 5715.73, NNZs: 4, Bias: 979.831388, T: 87744052, Avg. loss: 805372.377956\n",
      "Total training time: 10.96 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 5698.81, NNZs: 4, Bias: 978.809146, T: 88169994, Avg. loss: 786946.444320\n",
      "Total training time: 11.01 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 5681.99, NNZs: 4, Bias: 977.791934, T: 88595936, Avg. loss: 782187.559133\n",
      "Total training time: 11.07 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 5670.96, NNZs: 4, Bias: 976.861881, T: 89021878, Avg. loss: 780017.263149\n",
      "Total training time: 11.12 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 5654.97, NNZs: 4, Bias: 975.879530, T: 89447820, Avg. loss: 775581.691446\n",
      "Total training time: 11.17 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 5637.67, NNZs: 4, Bias: 974.867094, T: 89873762, Avg. loss: 761575.113082\n",
      "Total training time: 11.21 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 5621.54, NNZs: 4, Bias: 973.880623, T: 90299704, Avg. loss: 769238.375079\n",
      "Total training time: 11.25 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 5604.49, NNZs: 4, Bias: 972.869721, T: 90725646, Avg. loss: 760749.701054\n",
      "Total training time: 11.31 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 5586.54, NNZs: 4, Bias: 971.825761, T: 91151588, Avg. loss: 759373.355595\n",
      "Total training time: 11.35 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 5571.47, NNZs: 4, Bias: 970.833019, T: 91577530, Avg. loss: 759340.334401\n",
      "Total training time: 11.40 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 5556.15, NNZs: 4, Bias: 969.857198, T: 92003472, Avg. loss: 761455.374572\n",
      "Total training time: 11.46 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 5542.22, NNZs: 4, Bias: 968.867927, T: 92429414, Avg. loss: 749944.759514\n",
      "Total training time: 11.50 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 5525.58, NNZs: 4, Bias: 967.926279, T: 92855356, Avg. loss: 748475.015397\n",
      "Total training time: 11.56 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 5511.14, NNZs: 4, Bias: 966.972313, T: 93281298, Avg. loss: 733691.155543\n",
      "Total training time: 11.61 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 5495.76, NNZs: 4, Bias: 966.020771, T: 93707240, Avg. loss: 729374.556197\n",
      "Total training time: 11.65 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 5480.35, NNZs: 4, Bias: 965.102301, T: 94133182, Avg. loss: 738444.481233\n",
      "Total training time: 11.70 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 5469.02, NNZs: 4, Bias: 964.227212, T: 94559124, Avg. loss: 740714.184831\n",
      "Total training time: 11.74 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 5457.91, NNZs: 4, Bias: 963.293632, T: 94985066, Avg. loss: 735961.812285\n",
      "Total training time: 11.80 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 5443.45, NNZs: 4, Bias: 962.373401, T: 95411008, Avg. loss: 725662.003456\n",
      "Total training time: 11.84 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 5431.27, NNZs: 4, Bias: 961.479048, T: 95836950, Avg. loss: 718419.915127\n",
      "Total training time: 11.89 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 5416.94, NNZs: 4, Bias: 960.561747, T: 96262892, Avg. loss: 717488.571508\n",
      "Total training time: 11.94 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 5405.28, NNZs: 4, Bias: 959.647931, T: 96688834, Avg. loss: 718893.216721\n",
      "Total training time: 12.00 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 5391.52, NNZs: 4, Bias: 958.729075, T: 97114776, Avg. loss: 714163.994051\n",
      "Total training time: 12.05 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 5380.34, NNZs: 4, Bias: 957.852467, T: 97540718, Avg. loss: 715314.151345\n",
      "Total training time: 12.10 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 5366.47, NNZs: 4, Bias: 956.941900, T: 97966660, Avg. loss: 703667.506335\n",
      "Total training time: 12.14 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 5351.38, NNZs: 4, Bias: 956.015558, T: 98392602, Avg. loss: 710485.911398\n",
      "Total training time: 12.18 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 5340.75, NNZs: 4, Bias: 955.163454, T: 98818544, Avg. loss: 694865.817246\n",
      "Total training time: 12.23 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 5331.07, NNZs: 4, Bias: 954.339177, T: 99244486, Avg. loss: 712120.177747\n",
      "Total training time: 12.28 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 5319.90, NNZs: 4, Bias: 953.494548, T: 99670428, Avg. loss: 704845.174396\n",
      "Total training time: 12.33 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 5307.90, NNZs: 4, Bias: 952.634562, T: 100096370, Avg. loss: 679985.473918\n",
      "Total training time: 12.37 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 5295.06, NNZs: 4, Bias: 951.767983, T: 100522312, Avg. loss: 691492.110233\n",
      "Total training time: 12.42 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 5281.53, NNZs: 4, Bias: 950.905655, T: 100948254, Avg. loss: 684810.589932\n",
      "Total training time: 12.47 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 5269.13, NNZs: 4, Bias: 950.062594, T: 101374196, Avg. loss: 681070.834469\n",
      "Total training time: 12.52 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 5256.66, NNZs: 4, Bias: 949.189498, T: 101800138, Avg. loss: 680735.720199\n",
      "Total training time: 12.56 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 5243.52, NNZs: 4, Bias: 948.319970, T: 102226080, Avg. loss: 682064.172318\n",
      "Total training time: 12.61 seconds.\n",
      "Convergence after 240 epochs took 12.61 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='perceptron', max_iter=300, n_jobs=-1, verbose=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second try: use perceptron loss function\n",
    "sgd_perceptron = SGDClassifier(loss='perceptron', verbose=2, n_jobs=-1, max_iter=300)\n",
    "sgd_perceptron.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7502582499107864"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_perceptron.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
